{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfMqxGTykQalLWjdKbO6Mm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirMotefaker/Twitter-Watch/blob/main/Scrape_Twitter_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Scrape Twitter?\n",
        "\n",
        "- [Twitter](https://twitter.com/Twitter) is a major announcement hub where people and companies publish their announcements. This is a great opportunity to use Twitter to follow industry trends. For example, stock market or crypto market targets could be scraped to predict the future price of a stock or crypto.\n",
        "\n",
        "- Twitter is also a great source of data for sentiment analysis. You can use Twitter to find out what people think about a certain topic or brand. This is useful for market research, product development, and brand awareness.\n",
        "\n",
        "- So, if we can scrape Twitter data with Python we can have access to this valuable public information for free!"
      ],
      "metadata": {
        "id": "zOEizb9M8XZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Wizard\n",
        "\n",
        "- We'll approach Twitter scraping in three ways:\n",
        "\n",
        "  - We'll be using [browser automation toolkit Playwright](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/)\n",
        "    - This is the easiest way to scrape Twitter as we are using real web browser, so all we have to do is navigate to url, wait for page to load and get the results.\n",
        "\n",
        "  - We'll also take a look at reverse engineering Twitter's hidden API.\n",
        "This will be a bit harder but these type of scrapers will be much faster than the browser ones. For this we'll be using [httpx](https://pypi.org/project/httpx/).\n",
        "\n",
        "  - For ScrapFly users we'll also take a look at ScrapFly SDK which makes the above methods even easier.\n",
        "\n",
        "\n",
        "- We'll be working with both JSON and HTML response data. So, we'll be using [parsel](https://pypi.org/project/parsel/) to parse HTML and [jamespath for JSON](https://scrapfly.io/blog/parse-json-jmespath-python/)."
      ],
      "metadata": {
        "id": "Ll6VGmLI9vvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All of these libraries are available for free and can be installed via the pip install terminal command:\n",
        "\n",
        "   $ pip install httpx playwright parsel jmespath scrapfly-sdk\n",
        "\n"
      ],
      "metadata": {
        "id": "1rK2xyOO-6D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install httpx playwright parsel jmespath scrapfly-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUe2PT_t_Lev",
        "outputId": "34d4be29-6197-4ef0-f23c-93171ad8b8ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting playwright\n",
            "  Downloading playwright-1.31.1-py3-none-manylinux1_x86_64.whl (35.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parsel\n",
            "  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting jmespath\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting scrapfly-sdk\n",
            "  Downloading scrapfly_sdk-0.8.5-py3-none-any.whl (28 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx) (2022.12.7)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting greenlet==2.0.1\n",
            "  Downloading greenlet-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.9/535.9 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyee==9.0.4\n",
            "  Downloading pyee-9.0.4-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pyee==9.0.4->playwright) (4.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from parsel) (4.9.2)\n",
            "Collecting cssselect>=0.9\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting w3lib>=1.19.0\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from parsel) (23.0)\n",
            "Collecting backoff>=1.10.0\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (1.26.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (2.25.1)\n",
            "Requirement already satisfied: decorator>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (4.4.2)\n",
            "Collecting loguru>=0.5\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5.0,>=3.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->scrapfly-sdk) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.0->scrapfly-sdk) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.0->scrapfly-sdk) (4.0.0)\n",
            "Installing collected packages: rfc3986, w3lib, sniffio, pyee, loguru, jmespath, h11, greenlet, cssselect, backoff, scrapfly-sdk, playwright, parsel, anyio, httpcore, httpx\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 2.0.2\n",
            "    Uninstalling greenlet-2.0.2:\n",
            "      Successfully uninstalled greenlet-2.0.2\n",
            "Successfully installed anyio-3.6.2 backoff-2.2.1 cssselect-1.2.0 greenlet-2.0.1 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 jmespath-1.0.1 loguru-0.6.0 parsel-1.7.0 playwright-1.31.1 pyee-9.0.4 rfc3986-1.5.0 scrapfly-sdk-0.8.5 sniffio-1.3.0 w3lib-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Tweets\n",
        "\n",
        "- Twitter is a complicated javascript web application that requires javascript to work. So, for tweet scraping we'll be using [Playwright](https://playwright.dev/) browser automation library."
      ],
      "metadata": {
        "id": "A-vEhqMH_tpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Playwright-based Twitter scraper in Python should look something like this:\n",
        "\n",
        "  1- We'll start a headless Chrome browser\n",
        "\n",
        "  2- Navigate it to Tweet page URL like https://twitter.com/Scrapfly_dev/status/1577664612908077062\n",
        "\n",
        "  3- Wait for the page to load\n",
        "\n",
        "  4- Retrieve page HTML source\n",
        "\n",
        "  5- Load HTML to \"parsel.Selector\"\n",
        "\n",
        "  6- Use CSS selectors and XPath to extract Tweet details and replies"
      ],
      "metadata": {
        "id": "nqaN4_gHApMp"
      }
    }
  ]
}