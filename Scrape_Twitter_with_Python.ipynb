{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNy5m5uXYbKqCE7YamkslT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirMotefaker/Twitter-Watch/blob/main/Scrape_Twitter_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Scrape Twitter?\n",
        "\n",
        "- [Twitter](https://twitter.com/Twitter) is a major announcement hub where people and companies publish their announcements. This is a great opportunity to use Twitter to follow industry trends. For example, stock market or crypto market targets could be scraped to predict the future price of a stock or crypto.\n",
        "\n",
        "- Twitter is also a great source of data for sentiment analysis. You can use Twitter to find out what people think about a certain topic or brand. This is useful for market research, product development, and brand awareness.\n",
        "\n",
        "- So, if we can scrape Twitter data with Python we can have access to this valuable public information for free!"
      ],
      "metadata": {
        "id": "zOEizb9M8XZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Wizard\n",
        "\n",
        "- We'll approach Twitter scraping in three ways:\n",
        "\n",
        "  - We'll be using [browser automation toolkit Playwright](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/)\n",
        "    - This is the easiest way to scrape Twitter as we are using real web browser, so all we have to do is navigate to url, wait for page to load and get the results.\n",
        "\n",
        "  - We'll also take a look at reverse engineering Twitter's hidden API.\n",
        "This will be a bit harder but these type of scrapers will be much faster than the browser ones. For this we'll be using [httpx](https://pypi.org/project/httpx/).\n",
        "\n",
        "  - For ScrapFly users we'll also take a look at ScrapFly SDK which makes the above methods even easier.\n",
        "\n",
        "\n",
        "- We'll be working with both JSON and HTML response data. So, we'll be using [parsel](https://pypi.org/project/parsel/) to parse HTML and [jamespath for JSON](https://scrapfly.io/blog/parse-json-jmespath-python/)."
      ],
      "metadata": {
        "id": "Ll6VGmLI9vvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All of these libraries are available for free and can be installed via the pip install terminal command:\n",
        "\n",
        "   $ pip install httpx playwright parsel jmespath scrapfly-sdk\n",
        "\n"
      ],
      "metadata": {
        "id": "1rK2xyOO-6D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install httpx playwright parsel jmespath asyncio gevent scrapfly-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUe2PT_t_Lev",
        "outputId": "2df2bac0-6e4c-4e78-daa8-c7d078c77316"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.9/dist-packages (0.23.3)\n",
            "Requirement already satisfied: playwright in /usr/local/lib/python3.9/dist-packages (1.31.1)\n",
            "Requirement already satisfied: parsel in /usr/local/lib/python3.9/dist-packages (1.7.0)\n",
            "Requirement already satisfied: jmespath in /usr/local/lib/python3.9/dist-packages (1.0.1)\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.9/dist-packages (3.4.3)\n",
            "Requirement already satisfied: gevent in /usr/local/lib/python3.9/dist-packages (22.10.2)\n",
            "Requirement already satisfied: scrapfly-sdk in /usr/local/lib/python3.9/dist-packages (0.8.5)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx) (0.16.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx) (2022.12.7)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx) (1.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx) (1.3.0)\n",
            "Requirement already satisfied: greenlet==2.0.1 in /usr/local/lib/python3.9/dist-packages (from playwright) (2.0.1)\n",
            "Requirement already satisfied: pyee==9.0.4 in /usr/local/lib/python3.9/dist-packages (from playwright) (9.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pyee==9.0.4->playwright) (4.5.0)\n",
            "Requirement already satisfied: cssselect>=0.9 in /usr/local/lib/python3.9/dist-packages (from parsel) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from parsel) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from parsel) (23.0)\n",
            "Requirement already satisfied: w3lib>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from parsel) (2.1.1)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.9/dist-packages (from gevent) (5.5.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from gevent) (57.4.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.9/dist-packages (from gevent) (4.6)\n",
            "Requirement already satisfied: loguru>=0.5 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (0.6.0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (2.8.2)\n",
            "Requirement already satisfied: decorator>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (4.4.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (2.25.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.9/dist-packages (from scrapfly-sdk) (1.26.14)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx) (3.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->scrapfly-sdk) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.0->scrapfly-sdk) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.0->scrapfly-sdk) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Tweets\n",
        "\n",
        "- Twitter is a complicated javascript web application that requires javascript to work. So, for tweet scraping we'll be using [Playwright](https://playwright.dev/) browser automation library."
      ],
      "metadata": {
        "id": "A-vEhqMH_tpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Playwright-based Twitter scraper in Python should look something like this:\n",
        "\n",
        "  1- We'll start a headless Chrome browser\n",
        "\n",
        "  2- Navigate it to Tweet page URL like https://twitter.com/Scrapfly_dev/status/1577664612908077062\n",
        "\n",
        "  3- Wait for the page to load\n",
        "\n",
        "  4- Retrieve page HTML source\n",
        "\n",
        "  5- Load HTML to \"parsel.Selector\"\n",
        "\n",
        "  6- Use CSS selectors and XPath to extract Tweet details and replies"
      ],
      "metadata": {
        "id": "nqaN4_gHApMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python"
      ],
      "metadata": {
        "id": "EN5em7koErSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parsel import Selector\n",
        "from playwright.sync_api import sync_playwright\n",
        "from playwright.sync_api._generated import Page\n",
        "\n",
        "\n",
        "def parse_tweets(selector: Selector):\n",
        "    \"\"\"\n",
        "    parse tweets from pages containing tweets like:\n",
        "    - tweet page\n",
        "    - search page\n",
        "    - reply page\n",
        "    - homepage\n",
        "    returns list of tweets on the page where 1st tweet is the \n",
        "    main tweet and the rest are replies\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    # select all tweets on the page as individual boxes\n",
        "    # each tweet is stored under <article data-testid=\"tweet\"> box:\n",
        "    tweets = selector.xpath(\"//article[@data-testid='tweet']\")\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        # using data-testid attribute we can get tweet details:\n",
        "        found = {\n",
        "            \"text\": \"\".join(tweet.xpath(\".//*[@data-testid='tweetText']//text()\").getall()),\n",
        "            \"username\": tweet.xpath(\".//*[@data-testid='User-Names']/div[1]//text()\").get(),\n",
        "            \"handle\": tweet.xpath(\".//*[@data-testid='User-Names']/div[2]//text()\").get(),\n",
        "            \"datetime\": tweet.xpath(\".//time/@datetime\").get(),\n",
        "            \"verified\": bool(tweet.xpath(\".//svg[@data-testid='icon-verified']\")),\n",
        "            \"url\": tweet.xpath(\".//time/../@href\").get(),\n",
        "            \"image\": tweet.xpath(\".//*[@data-testid='tweetPhoto']/img/@src\").get(),\n",
        "            \"video\": tweet.xpath(\".//video/@src\").get(),\n",
        "            \"video_thumb\": tweet.xpath(\".//video/@poster\").get(),\n",
        "            \"likes\": tweet.xpath(\".//*[@data-testid='like']//text()\").get(),\n",
        "            \"retweets\": tweet.xpath(\".//*[@data-testid='retweet']//text()\").get(),\n",
        "            \"replies\": tweet.xpath(\".//*[@data-testid='reply']//text()\").get(),\n",
        "            \"views\": (tweet.xpath(\".//*[contains(@aria-label,'Views')]\").re(\"(\\d+) Views\") or [None])[0],\n",
        "        }\n",
        "        # main tweet (not a reply):\n",
        "        if i == 0:\n",
        "            found[\"views\"] = tweet.xpath('.//span[contains(text(),\"Views\")]/../preceding-sibling::div//text()').get()\n",
        "            found[\"retweets\"] = tweet.xpath('.//a[contains(@href,\"retweets\")]//text()').get()\n",
        "            found[\"quote_tweets\"] = tweet.xpath('.//a[contains(@href,\"retweets/with_comments\")]//text()').get()\n",
        "            found[\"likes\"] = tweet.xpath('.//a[contains(@href,\"likes\")]//text()').get()\n",
        "        results.append({k: v for k, v in found.items() if v is not None})\n",
        "    return results\n",
        "\n",
        "\n",
        "def scrape_tweet(url: str, page: Page):\n",
        "    \"\"\"\n",
        "    Scrape tweet and replies from tweet page like:\n",
        "    https://twitter.com/Scrapfly_dev/status/1587431468141318146\n",
        "    \"\"\"\n",
        "    # go to url\n",
        "    page.goto(url)\n",
        "    # wait for content to load\n",
        "    page.wait_for_selector(\"//article[@data-testid='tweet']\")  \n",
        "    # retrieve final page HTML:\n",
        "    html = page.content()\n",
        "    # parse it for data:\n",
        "    selector = Selector(html)\n",
        "    tweets = parse_tweets(selector)\n",
        "    return tweets\n",
        "\n",
        "\n",
        "# # example run:\n",
        "# with sync_playwright() as pw:\n",
        "#     # start browser and open a new tab:\n",
        "#     browser = pw.chromium.launch(headless=False)\n",
        "#     page = browser.new_page(viewport={\"width\": 1920, \"height\": 1080})\n",
        "#     # scrape tweet and replies:\n",
        "#     tweet_and_replies = scrape_tweet(\"httpTrutwitter.com/Scrapfly_dev/status/1587431468141318146\", page)\n",
        "#     print(tweet_and_replies)\n",
        "\n",
        "\n",
        "# example\n",
        "from playwright.async_api import async_playwright # need to import this first\n",
        "from gevent import monkey, spawn\n",
        "import asyncio\n",
        "import gevent\n",
        "\n",
        "monkey.patch_all()\n",
        "loop = asyncio.new_event_loop()\n",
        "\n",
        "\n",
        "async def f():\n",
        "    print(\"start\")\n",
        "    playwright = await async_playwright().start()\n",
        "    browser = await playwright.chromium.launch(headless=True)\n",
        "    context = await browser.new_context()\n",
        "    page = await context.new_page()\n",
        "    await page.goto(f\"https://www.google.com\")\n",
        "    print(\"done\")\n",
        "\n",
        "\n",
        "def greeny():\n",
        "    while True:  # and not other_exit_condition\n",
        "        future = asyncio.run_coroutine_threadsafe(f(), loop)\n",
        "        while not future.done():\n",
        "            gevent.sleep(1)\n",
        "\n",
        "\n",
        "greenlet1 = spawn(greeny)\n",
        "greenlet2 = spawn(greeny)\n",
        "#loop.run_forever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVjP-KHTEtAm",
        "outputId": "b4eb4956-823e-455c-8301-0cfd6603667d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It seems that the gevent monkey-patching is being used.\n",
            "Please set an environment variable with:\n",
            "GEVENT_SUPPORT=True\n",
            "to enable gevent support in the debugger.\n"
          ]
        }
      ]
    }
  ]
}