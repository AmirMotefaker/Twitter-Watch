{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp8tsVfivJHevwsyLQrfsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirMotefaker/Twitter-Watch/blob/main/Twitter_Sentiment_Analysis_using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is sentiment analysis? \n",
        "- Sentiment analysis is contextual mining of text which identifies and extracts subjective information in the source material and helps a business to understand the social sentiment of their brand, product, or service while monitoring online conversations. However, analysis of social media streams is usually restricted to just basic sentiment analysis and count-based metrics.\n",
        "\n",
        "- Sentiment analysis is an artificial intelligence (AI)-based capability that uses machine learning to recognize sentiments and assess the emotional content of texts and images. It is a layer of understanding applied to the rest of your market research and social analytics that puts data analytics into context. And it offers meaningful insights to drive everything from product innovation and packaging to messaging and maintaining a competitive edge. It categorizes consumer emotions and overarching market movements by type and intensity.\n",
        "\n",
        "- Social listening, social monitoring, image analytics, and customer experience analytics – all of these rely on valuable and accurate sentiment analysis.\n",
        "\n",
        "\n",
        "# The Different Types of Sentiment Analysis\n",
        "- Machine learning-based sentiment analysis: A machine learning algorithm learns from the data as it goes. It requires and is dependent upon its training and builds upon itself. If it becomes baked into the sentiment analysis algorithm, it learns something incorrectly. Machine learning models are often trained on very questionable datasets that typically contain little to no social channel data, making them an ill-advised option for understanding sentiment. Although it has made great strides in handling low-level tasks, when it comes to understanding the meaning of human language, there are still many problems with machine learning-based sentiment analysis and many ways in which it is inferior to rule-based systems.\n",
        "\n",
        "- Rule-based sentiment analysis: This sentiment analysis model consists of manually created rules that count things up for an aggregated score. It sounds simplistic, and it is, but it’s also effective (to a degree). But it has limitations: “The result of this approach is a set of rules based on which the text is labeled as neutral, positive and negative words. These rules are also known as lexicons. Hence, the Rule-based approach is also called the Lexicon-based approach.”\n",
        "\n",
        "- Aspect-based sentiment analysis: This categorizes data by aspect and identifies the sentiment attributed to each. It associates specific sentiments with different aspects shared. For example, ”I want to go swimming so much.” It is clear that the person has positive feelings about swimming, as evidenced by their desire to frequent the location. Sentiment analysis identifies positive sentiment on the part of the subject towards swimming. We will discuss aspect-based sentiment analysis below and as we go, so keep reading!\n",
        "\n",
        "\n",
        "# Five ways a sentiment analysis system can help us:\n",
        "- Better Audience Understanding\n",
        "  - Remember that your mentions—whether positive, negative, or neutral—don’t occur in a vacuum. Instead of concentrating on a single compliment or complaint, brands should consider the overall sentiment of their audience. Every brand loves a barrage of compliments, which can help you understand what you’re doing right.\n",
        "\n",
        "- Provides Actionable Insights\n",
        "  - Consumers love to tag and talk about brands online, making a social media sentiment analysis ideal due to the wealth of information available. You can get accurate data points demonstrating how your brand performs over time and across all platforms.\n",
        "\n",
        "- Meet your consumers where they are\n",
        "  - Because consumers are talking online, brands are given the distinct opportunity to meet them where they are and with precisely what they desire. You can engage directly with your audience, answer their questions, or even respond to criticism promptly. Your brand’s health is protected by solving customers’ problems or not letting a consumer complaint fester.\n",
        "\n",
        "- More profound insights about your brand, its messaging, and more\n",
        "  - You can modify your brand messaging for a more significant impact when you account for how users interact with your brand and the type of content they value. A sentiment analysis system allows you to see what consumers love and hate.\n",
        "\n",
        "- Understand How You Measure Up in Your Industry\n",
        "  - Brands can’t be everything to everyone. You can use social sentiment to gauge your position in your industry. Your ability to communicate effectively with the appropriate audiences at the proper times will benefit from this.\n",
        "\n",
        "\n",
        "# Text Classifier — The basic building blocks\n",
        "\n",
        "## Sentiment Analysis\n",
        "- Sentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative, or neutral. \n",
        "\n",
        "## Intent Analysis\n",
        "- Intent analysis steps up the game by analyzing the user’s intention behind a message and identifying whether it relates to an opinion, news, marketing, complaint, suggestion, appreciation, or query.\n",
        "\n",
        "## Contextual Semantic Search(CSS)\n",
        "- To derive actionable insights, it is important to understand what aspect of the brand is a user discussing. For example, Amazon would want to segregate messages related to late deliveries, billing issues, promotion-related queries, product reviews, etc. On the other hand, Starbucks would want to classify messages based on whether they relate to staff behavior, new coffee flavors, hygiene feedback, online orders, store name, and location, etc. But how can one do that?\n",
        "\n",
        "- We introduce an intelligent smart search algorithm called Contextual Semantic Search (a.k.a. CSS). The way CSS works are that it takes thousands of messages and a concept (like Price) as input and filters all the messages that closely match the given concept.\n",
        "\n",
        "- An AI technique is used to convert every word into a specific point in the hyperspace and the distance between these points is used to identify messages where the context is similar to the concept are exploring.\n",
        "\n",
        "- Ref1: [What is Sentiment Analysis?](https://aws.amazon.com/what-is/sentiment-analysis/)\n",
        "- Ref2: [Sentiment Analysis: Concept, Analysis and Applications](https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)"
      ],
      "metadata": {
        "id": "UpGaz-W1Gk0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "zs163LGfdFJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Tweepy](https://github.com/tweepy/tweepy)\n",
        "\n",
        "  -  tweepy is the python client for the official [Twitter API](https://dev.twitter.com/rest/public). \n",
        "\n",
        "  - [Tweepy latest version documentation](https://docs.tweepy.org/en/v4.12.1/)\n",
        "\n",
        "\n",
        "  - Install it using the following pip command:\n",
        "\n",
        "      - pip install tweepy\n",
        "      \n",
        "\n",
        "\n",
        "- [Documentation](https://tweepy.readthedocs.io/en/latest/)\n",
        "\n",
        "- [Official Discord Server](https://discord.gg/bJvqnhg)\n",
        "\n",
        "- [Twitter API Documentation](https://developer.twitter.com/en/docs/twitter-api)"
      ],
      "metadata": {
        "id": "wxIbF5gfg7kZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [TextBlob](https://github.com/sloria/TextBlob)\n",
        "\n",
        "  - textblob is a Simple, Python, text processing--Sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and more.\n",
        "\n",
        "  - [TextBlob documentation](https://textblob.readthedocs.io/en/dev/)\n",
        "\n",
        "\n",
        "  - Install it using the following pip command:\n",
        "\n",
        "      - pip install -U textblob\n",
        "      \n",
        "\n",
        "\n",
        "- Docs: https://textblob.readthedocs.io/\n",
        "\n",
        "- Changelog: https://textblob.readthedocs.io/en/latest/changelog.html\n",
        "\n",
        "- PyPI: https://pypi.python.org/pypi/TextBlob\n",
        "\n",
        "- Issues: https://github.com/sloria/TextBlob/issues"
      ],
      "metadata": {
        "id": "eg31-Jf-g9Mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK\n",
        "- We need to install some NLTK corpora, Corpora are nothing but a large and structured set of texts. Use the following command to install NLTK corpora :\n",
        "  - python -m textblob.download_corpora\n",
        "\n",
        "- [Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)"
      ],
      "metadata": {
        "id": "t6SShIl_oWYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter API\n",
        "\n",
        "- Open this [link](https://apps.twitter.com/) and click the button: ‘Create New App’\n",
        "- Fill the application details. You can leave the callback url field empty.\n",
        "- Once the app is created, you will be redirected to the app page.\n",
        "- Open the ‘Keys and Access Tokens’ tab.\n",
        "- Copy ‘Consumer Key’, ‘Consumer Secret’, ‘Access token’ and ‘Access Token Secret’."
      ],
      "metadata": {
        "id": "SXRw5jCcotTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import re # A Regular Expression (RegEx) is a sequence of characters that defines a search pattern.\n",
        "# import tweepy\n",
        "# from tweepy import OAuthHandler\n",
        "# from textblob import TextBlob\n",
        " \n",
        "# class TwitterClient(object):\n",
        "#     '''\n",
        "#     Generic Twitter Class for sentiment analysis.\n",
        "#     '''\n",
        "#     def __init__(self):\n",
        "#         '''\n",
        "#         Class constructor or initialization method.\n",
        "#         '''\n",
        "#         # keys and tokens from the Twitter Dev Console\n",
        "#         consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXX'\n",
        "#         consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
        "#         access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
        "#         access_token_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
        " \n",
        "#         # attempt authentication\n",
        "#         try:\n",
        "#             # create OAuthHandler object\n",
        "#             self.auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "#             # set access token and secret\n",
        "#             self.auth.set_access_token(access_token, access_token_secret)\n",
        "#             # create tweepy API object to fetch tweets\n",
        "#             self.api = tweepy.API(self.auth)\n",
        "#         except:\n",
        "#             print(\"Error: Authentication Failed\")\n",
        " \n",
        "#     def clean_tweet(self, tweet):\n",
        "#         '''\n",
        "#         Utility function to clean tweet text by removing links, special characters\n",
        "#         using simple regex statements.\n",
        "#         '''\n",
        "#         return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\n",
        "#                                     |(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        " \n",
        "#     def get_tweet_sentiment(self, tweet):\n",
        "#         '''\n",
        "#         Utility function to classify sentiment of passed tweet\n",
        "#         using textblob's sentiment method\n",
        "#         '''\n",
        "#         # create TextBlob object of passed tweet text\n",
        "#         analysis = TextBlob(self.clean_tweet(tweet))\n",
        "#         # set sentiment\n",
        "#         if analysis.sentiment.polarity > 0:\n",
        "#             return 'positive'\n",
        "#         elif analysis.sentiment.polarity == 0:\n",
        "#             return 'neutral'\n",
        "#         else:\n",
        "#             return 'negative'\n",
        " \n",
        "#     def get_tweets(self, query, count = 10):\n",
        "#         '''\n",
        "#         Main function to fetch tweets and parse them.\n",
        "#         '''\n",
        "#         # empty list to store parsed tweets\n",
        "#         tweets = []\n",
        " \n",
        "#         try:\n",
        "#             # call twitter api to fetch tweets\n",
        "#             fetched_tweets = self.api.search(q = query, count = count)\n",
        " \n",
        "#             # parsing tweets one by one\n",
        "#             for tweet in fetched_tweets:\n",
        "#                 # empty dictionary to store required params of a tweet\n",
        "#                 parsed_tweet = {}\n",
        " \n",
        "#                 # saving text of tweet\n",
        "#                 parsed_tweet['text'] = tweet.text\n",
        "#                 # saving sentiment of tweet\n",
        "#                 parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
        " \n",
        "#                 # appending parsed tweet to tweets list\n",
        "#                 if tweet.retweet_count > 0:\n",
        "#                     # if tweet has retweets, ensure that it is appended only once\n",
        "#                     if parsed_tweet not in tweets:\n",
        "#                         tweets.append(parsed_tweet)\n",
        "#                 else:\n",
        "#                     tweets.append(parsed_tweet)\n",
        " \n",
        "#             # return parsed tweets\n",
        "#             return tweets\n",
        " \n",
        "#         except tweepy.TweepError as e:\n",
        "#             # print error (if any)\n",
        "#             print(\"Error : \" + str(e))\n",
        " \n",
        "# def main():\n",
        "#     # creating object of TwitterClient Class\n",
        "#     api = TwitterClient()\n",
        "#     # calling function to get tweets\n",
        "#     tweets = api.get_tweets(query = 'Donald Trump', count = 200)\n",
        " \n",
        "#     # picking positive tweets from tweets\n",
        "#     ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive']\n",
        "#     # percentage of positive tweets\n",
        "#     print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets)))\n",
        "#     # picking negative tweets from tweets\n",
        "#     ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']\n",
        "#     # percentage of negative tweets\n",
        "#     print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets)))\n",
        "#     # percentage of neutral tweets\n",
        "#     print(\"Neutral tweets percentage: {} % \\\n",
        "#         \".format(100*(len(tweets) -(len( ntweets )+len( ptweets)))/len(tweets)))\n",
        " \n",
        "#     # printing first 5 positive tweets\n",
        "#     print(\"\\n\\nPositive tweets:\")\n",
        "#     for tweet in ptweets[:10]:\n",
        "#         print(tweet['text'])\n",
        " \n",
        "#     # printing first 5 negative tweets\n",
        "#     print(\"\\n\\nNegative tweets:\")\n",
        "#     for tweet in ntweets[:10]:\n",
        "#         print(tweet['text'])\n",
        " \n",
        "# if __name__ == \"__main__\":\n",
        "#     # calling main function\n",
        "#     main()"
      ],
      "metadata": {
        "id": "a_oMHfjEpXQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting Drive to Colab"
      ],
      "metadata": {
        "id": "8SqWHBtapwIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEFamx-dpyIF",
        "outputId": "6739be49-2160-49cc-d376-6dd7980312bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up PySpark in Colab"
      ],
      "metadata": {
        "id": "vs80SggFrYjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "U74hdCFYrZrh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will install Apache Spark with Hadoop from [here](http://spark.apache.org/downloads.html).\n"
      ],
      "metadata": {
        "id": "JBjdSTs_sH0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget -q https://www.apache.org/dyn/closer.lua/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "VQGnl9ZSsOnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just need to unzip that folder"
      ],
      "metadata": {
        "id": "8vehantIwQyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!tar xf '/content/drive/MyDrive/Colab Notebooks/spark-3.3.2-bin-hadoop3.gz'"
      ],
      "metadata": {
        "id": "KR10yrJcwTx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There is one last thing that we need to install and that is the [findspark](https://pypi.org/project/findspark/) library. It will locate Spark on the system and import it as a regular library."
      ],
      "metadata": {
        "id": "Yw0lAIssCe5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "eG26yNwgCjhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment."
      ],
      "metadata": {
        "id": "iHzGPCrbD3kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\""
      ],
      "metadata": {
        "id": "nC3oGUilD4lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method."
      ],
      "metadata": {
        "id": "QbYgO5yYUQ8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "fIMIiOIeUSCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If you want to know the location where Spark is installed, use findspark.find()"
      ],
      "metadata": {
        "id": "IuPQoxrhVc84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "avpL4yXWVeXL",
        "outputId": "70fc9166-e6e8-4611-daf1-1bc63d1e9a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.3.2-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can import [SparkSession](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.SparkSession) from [pyspark.sql](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark-sql-module) and create a SparkSession, which is the entry point to Spark.\n",
        "\n",
        "- You can give a name to the session using appName() and add some configurations with config() if you wish."
      ],
      "metadata": {
        "id": "qO9aVDIZVpnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "LzItswWxVzmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Print the SparkSession variable:"
      ],
      "metadata": {
        "id": "QtqOPkmaWA_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "UOKYsuvHWCvT",
        "outputId": "5a680b82-96ad-4e3b-8e8f-1a960e82c013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f450aac57f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://01615373df52:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If everything goes well, you should be able to view the above output.\n",
        "\n",
        "- If you want to view the Spark UI, you would have to include a few more lines of code to create a public URL for the UI page:"
      ],
      "metadata": {
        "id": "Sz5o8hWGWdCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcYHXsyzWftf",
        "outputId": "4324ea96-815f-4a2e-a071-5caea4af072c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-05 11:01:20--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 52.202.168.65, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.6’\n",
            "\n",
            "\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \rngrok-stable-linux- 100%[===================>]  13.28M  73.5MB/s    in 0.2s    \n",
            "\n",
            "2023-03-05 11:01:21 (73.5 MB/s) - ‘ngrok-stable-linux-amd64.zip.6’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "{\"tunnels\":[],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now you should be able to view the jobs and their stages at the link created."
      ],
      "metadata": {
        "id": "rQME4vNbWp3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Tweets"
      ],
      "metadata": {
        "id": "DKTZmsv3aZZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyspark\n",
        "# !pip install findspark\n",
        "# !pip install py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X543qmaSaaWM",
        "outputId": "1db66bea-9805-4c88-f863-94f2e526eb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.8/dist-packages (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.8/dist-packages (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import desc\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from IPython import display\n",
        "from PIL import Image\n",
        "from os import path, getcwd\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "'''\n",
        "Add the new values of counts with the previous running count to get the latest running count\n",
        "'''  \n",
        "def updateFunction(newValues, runningCount):\n",
        "    if runningCount is None:\n",
        "        runningCount = 0\n",
        "    return sum(newValues, runningCount)  "
      ],
      "metadata": {
        "id": "QS0NcUk5atX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capturing the Tweet Stream"
      ],
      "metadata": {
        "id": "m-SWlMtFa2h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext()\n",
        "'''\n",
        "Create a StreamingContext with batch interval of 10 seconds\n",
        "'''\n",
        "ssc = StreamingContext(sc, 10)\n",
        "ssc.checkpoint('twitterStreamCheckpoint')\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "def getWordList(fileName):\n",
        "    ''' \n",
        "    This function returns a list of words from the given file name.\n",
        "    '''\n",
        "    lines = sc.textFile(fileName)\n",
        "    '''\n",
        "    Filtering blank lines and lines that start with ';' from the input file\n",
        "    '''\n",
        "    lines=lines.filter(lambda line : line and line[0].isalpha())\n",
        "    words = lines.flatMap(lambda line:line.split('\\n'))\n",
        "    return words.collect()\n",
        "\n",
        "'''\n",
        "Creating positive and negative word list using getWordList function.\n",
        "'''\n",
        "positiveWordsList = getWordList('positive_words.txt')\n",
        "negativeWordsList = getWordList('negative_words.txt')\n",
        "\n",
        "\n",
        "socketStream = ssc.socketTextStream('127.0.0.1', 9999)\n",
        "tweets = socketStream.window(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "rG-5d9K8a4Um",
        "outputId": "2d4f1146-2bfb-4e82-a2f8-550e9aee96d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ff68cd1e9035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m '''\n\u001b[1;32m      3\u001b[0m \u001b[0mCreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mStreamingContext\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0minterval\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m '''\n\u001b[1;32m      5\u001b[0m \u001b[0mssc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/content/spark-3.3.2-bin-hadoop3/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    431\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Colab, master=local) created by getOrCreate at <ipython-input-7-660d79bf4732>:3 "
          ]
        }
      ]
    }
  ]
}