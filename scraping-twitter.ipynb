{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/scraping-twitter?scriptVersionId=121835405\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"85c76533","metadata":{"papermill":{"duration":0.00217,"end_time":"2023-03-12T06:21:09.871827","exception":false,"start_time":"2023-03-12T06:21:09.869657","status":"completed"},"tags":[]},"source":["# Why Scrape Twitter?\n","\n","- [Twitter](https://twitter.com/Twitter) is a major announcement hub where people and companies publish their announcements. This is a great opportunity to use Twitter to follow industry trends. For example, stock market or crypto market targets could be scraped to predict the future price of a stock or crypto.\n","\n","- Twitter is also a great source of data for sentiment analysis. You can use Twitter to find out what people think about a certain topic or brand. This is useful for market research, product development, and brand awareness.\n","\n","- So, if we can scrape Twitter data with Python we can have access to this valuable public information for free!"]},{"cell_type":"markdown","id":"a06a7655","metadata":{"papermill":{"duration":0.001312,"end_time":"2023-03-12T06:21:09.874806","exception":false,"start_time":"2023-03-12T06:21:09.873494","status":"completed"},"tags":[]},"source":["# Setup Wizard\n","\n","- We'll approach Twitter scraping in three ways:\n","\n","  - We'll be using [browser automation toolkit Playwright](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/)\n","    - This is the easiest way to scrape Twitter as we are using real web browser, so all we have to do is navigate to url, wait for page to load and get the results.\n","\n","  - We'll also take a look at reverse engineering Twitter's hidden API.\n","This will be a bit harder but these type of scrapers will be much faster than the browser ones. For this we'll be using [httpx](https://pypi.org/project/httpx/).\n","\n","  - For ScrapFly users we'll also take a look at ScrapFly SDK which makes the above methods even easier.\n","\n","\n","- We'll be working with both JSON and HTML response data. So, we'll be using [parsel](https://pypi.org/project/parsel/) to parse HTML and [jamespath for JSON](https://scrapfly.io/blog/parse-json-jmespath-python/)."]},{"cell_type":"markdown","id":"a20f5591","metadata":{"papermill":{"duration":0.001246,"end_time":"2023-03-12T06:21:09.877546","exception":false,"start_time":"2023-03-12T06:21:09.8763","status":"completed"},"tags":[]},"source":["#### All of these libraries are available for free and can be installed via the pip install terminal command:\n","\n","   $ pip install httpx playwright parsel jmespath scrapfly-sdk"]},{"cell_type":"code","execution_count":1,"id":"e26e1689","metadata":{"execution":{"iopub.execute_input":"2023-03-12T06:21:09.883295Z","iopub.status.busy":"2023-03-12T06:21:09.881878Z","iopub.status.idle":"2023-03-12T06:21:36.834037Z","shell.execute_reply":"2023-03-12T06:21:36.832902Z"},"papermill":{"duration":26.957669,"end_time":"2023-03-12T06:21:36.836636","exception":false,"start_time":"2023-03-12T06:21:09.878967","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Scrapy\r\n","  Downloading Scrapy-2.8.0-py2.py3-none-any.whl (272 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.9/272.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting w3lib>=1.17.0\r\n","  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\r\n","Requirement already satisfied: pyOpenSSL>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (23.0.0)\r\n","Collecting itemadapter>=0.1.0\r\n","  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\r\n","Collecting queuelib>=1.4.2\r\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\r\n","Collecting service-identity>=18.1.0\r\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\r\n","Collecting protego>=0.1.15\r\n","  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\r\n","Collecting Twisted>=18.9.0\r\n","  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: lxml>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (4.9.2)\r\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from Scrapy) (23.0)\r\n","Collecting parsel>=1.5.0\r\n","  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\r\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from Scrapy) (59.8.0)\r\n","Collecting zope.interface>=5.1.0\r\n","  Downloading zope.interface-5.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting PyDispatcher>=2.0.5\r\n","  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\r\n","Requirement already satisfied: cryptography>=3.4.6 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (38.0.2)\r\n","Collecting tldextract\r\n","  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting cssselect>=0.9.1\r\n","  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\r\n","Collecting itemloaders>=1.0.1\r\n","  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\r\n","Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=3.4.6->Scrapy) (1.15.1)\r\n","Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.7/site-packages (from itemloaders>=1.0.1->Scrapy) (1.0.1)\r\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protego>=0.1.15->Scrapy) (1.16.0)\r\n","Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->Scrapy) (0.4.8)\r\n","Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->Scrapy) (0.2.8)\r\n","Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->Scrapy) (22.2.0)\r\n","Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from Twisted>=18.9.0->Scrapy) (4.4.0)\r\n","Collecting incremental>=21.3.0\r\n","  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\r\n","Collecting hyperlink>=17.1.1\r\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting constantly>=15.1\r\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\r\n","Collecting Automat>=0.8.0\r\n","  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\r\n","Collecting requests-file>=1.4\r\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\r\n","Requirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from tldextract->Scrapy) (3.9.0)\r\n","Requirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from tldextract->Scrapy) (3.4)\r\n","Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tldextract->Scrapy) (2.28.2)\r\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=3.4.6->Scrapy) (2.21)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2022.12.7)\r\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->Scrapy) (1.26.14)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.1.1)\r\n","Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, Scrapy\r\n","Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Scrapy-2.8.0 Twisted-22.10.0 constantly-15.1.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 parsel-1.7.0 protego-0.2.1 queuelib-1.6.2 requests-file-1.5.1 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.1.1 zope.interface-5.5.2\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0mCollecting httpx\r\n","  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting playwright\r\n","  Downloading playwright-1.31.1-py3-none-manylinux1_x86_64.whl (35.0 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: parsel in /opt/conda/lib/python3.7/site-packages (1.7.0)\r\n","Requirement already satisfied: jmespath in /opt/conda/lib/python3.7/site-packages (1.0.1)\r\n","Collecting asyncio\r\n","  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting gevent\r\n","  Downloading gevent-22.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting scrapfly-sdk\r\n","  Downloading scrapfly_sdk-0.8.5-py3-none-any.whl (28 kB)\r\n","Collecting httpcore<0.17.0,>=0.15.0\r\n","  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\r\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\r\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx) (2022.12.7)\r\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.7/site-packages (from httpx) (1.3.0)\r\n","Requirement already satisfied: greenlet==2.0.1 in /opt/conda/lib/python3.7/site-packages (from playwright) (2.0.1)\r\n","Collecting pyee==9.0.4\r\n","  Downloading pyee-9.0.4-py2.py3-none-any.whl (14 kB)\r\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from playwright) (4.4.0)\r\n","Requirement already satisfied: w3lib>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from parsel) (2.1.1)\r\n","Requirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.7/site-packages (from parsel) (1.2.0)\r\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from parsel) (23.0)\r\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from parsel) (4.9.2)\r\n","Collecting zope.event\r\n","  Downloading zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\r\n","Requirement already satisfied: zope.interface in /opt/conda/lib/python3.7/site-packages (from gevent) (5.5.2)\r\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from gevent) (59.8.0)\r\n","Requirement already satisfied: decorator>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (5.1.1)\r\n","Requirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (2.28.2)\r\n","Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (1.10.0)\r\n","Collecting loguru>=0.5\r\n","  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (1.26.14)\r\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (2.8.2)\r\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.7/site-packages (from httpcore<0.17.0,>=0.15.0->httpx) (0.14.0)\r\n","Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from httpcore<0.17.0,>=0.15.0->httpx) (3.6.2)\r\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->scrapfly-sdk) (1.16.0)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.25.0->scrapfly-sdk) (2.1.1)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.25.0->scrapfly-sdk) (3.4)\r\n","Installing collected packages: rfc3986, asyncio, zope.event, pyee, loguru, scrapfly-sdk, playwright, httpcore, gevent, httpx\r\n","Successfully installed asyncio-3.4.3 gevent-22.10.2 httpcore-0.16.3 httpx-0.23.3 loguru-0.6.0 playwright-1.31.1 pyee-9.0.4 rfc3986-1.5.0 scrapfly-sdk-0.8.5 zope.event-4.6\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["!pip install Scrapy\n","!pip install httpx playwright parsel jmespath asyncio gevent scrapfly-sdk"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":35.907384,"end_time":"2023-03-12T06:21:37.462525","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-03-12T06:21:01.555141","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}