{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/scraping-twitter?scriptVersionId=129757532\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Why Scrape Twitter?\n\n- [Twitter](https://twitter.com/Twitter) is a major announcement hub where people and companies publish their announcements. This is a great opportunity to use Twitter to follow industry trends. For example, stock market or crypto market targets could be scraped to predict the future price of a stock or crypto.\n\n- Twitter is also a great source of data for sentiment analysis. You can use Twitter to find out what people think about a certain topic or brand. This is useful for market research, product development, and brand awareness.\n\n- So, if we can scrape Twitter data with Python we can have access to this valuable public information for free!","metadata":{}},{"cell_type":"markdown","source":"# Twitter Scraping in 2023\n\n- Twitter is one of the most popular social media platforms, with millions of users tweeting and sharing their thoughts and opinions every day. As a result, Twitter has become a valuable source of data for businesses, researchers, and developers who want to analyze user behavior, sentiment, and trends.\n\n- The best way to get the data from Twitter is to use their official API. However, Twitter's API is becoming more and more restrictive: you can only get a limited amount of data from Twitter using their API. On top of that even the lowest tier of the API will require a monthly payment soon. That's why many people are looking for ways to scrape Twitter without these restrictions.\n\n[webscraping.ai](https://webscraping.ai/blog/twitter-scraping-in-2023)","metadata":{}},{"cell_type":"markdown","source":"# Setup Wizard\n\n- We'll approach Twitter scraping in three ways:\n\n  - We'll be using [browser automation toolkit Playwright](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/)\n    - This is the easiest way to scrape Twitter as we are using real web browser, so all we have to do is navigate to url, wait for page to load and get the results.\n\n  - We'll also take a look at reverse engineering Twitter's hidden API.\nThis will be a bit harder but these type of scrapers will be much faster than the browser ones. For this we'll be using [httpx](https://pypi.org/project/httpx/).\n\n  - For ScrapFly users we'll also take a look at ScrapFly SDK which makes the above methods even easier.\n\n\n- We'll be working with both JSON and HTML response data. So, we'll be using [parsel](https://pypi.org/project/parsel/) to parse HTML and [jamespath for JSON](https://scrapfly.io/blog/parse-json-jmespath-python/).","metadata":{}},{"cell_type":"markdown","source":"#### All of these libraries are available for free and can be installed via the pip install terminal command:\n\n   $ pip install httpx playwright parsel jmespath scrapfly-sdk","metadata":{}},{"cell_type":"code","source":"!pip install Scrapy\n!pip install httpx playwright parsel jmespath asyncio gevent scrapfly-sdk","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:55:49.070538Z","iopub.execute_input":"2023-03-13T13:55:49.07169Z","iopub.status.idle":"2023-03-13T13:56:15.176111Z","shell.execute_reply.started":"2023-03-13T13:55:49.071641Z","shell.execute_reply":"2023-03-13T13:56:15.17446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scraping Tweets\n\n- Twitter is a complicated javascript web application that requires javascript to work. So, for tweet scraping we'll be using [Playwright](https://playwright.dev/) browser automation library.","metadata":{}},{"cell_type":"markdown","source":"#### Playwright-based Twitter scraper in Python should look something like this:\n\n  1- We'll start a headless Chrome browser\n\n  2- Navigate it to Tweet page URL like https://twitter.com/Scrapfly_dev/status/1577664612908077062\n\n  3- Wait for the page to load\n\n  4- Retrieve page HTML source\n\n  5- Load HTML to \"parsel.Selector\"\n\n  6- Use CSS selectors and XPath to extract Tweet details and replies","metadata":{}},{"cell_type":"markdown","source":"# Python","metadata":{}},{"cell_type":"code","source":"from parsel import Selector\nfrom playwright.sync_api import sync_playwright\nfrom playwright.sync_api._generated import Page\n\n\ndef parse_tweets(selector: Selector):\n    \"\"\"\n    parse tweets from pages containing tweets like:\n    - tweet page\n    - search page\n    - reply page\n    - homepage\n    returns list of tweets on the page where 1st tweet is the \n    main tweet and the rest are replies\n    \"\"\"\n    results = []\n    # select all tweets on the page as individual boxes\n    # each tweet is stored under <article data-testid=\"tweet\"> box:\n    tweets = selector.xpath(\"//article[@data-testid='tweet']\")\n    for i, tweet in enumerate(tweets):\n        # using data-testid attribute we can get tweet details:\n        found = {\n            \"text\": \"\".join(tweet.xpath(\".//*[@data-testid='tweetText']//text()\").getall()),\n            \"username\": tweet.xpath(\".//*[@data-testid='User-Names']/div[1]//text()\").get(),\n            \"handle\": tweet.xpath(\".//*[@data-testid='User-Names']/div[2]//text()\").get(),\n            \"datetime\": tweet.xpath(\".//time/@datetime\").get(),\n            \"verified\": bool(tweet.xpath(\".//svg[@data-testid='icon-verified']\")),\n            \"url\": tweet.xpath(\".//time/../@href\").get(),\n            \"image\": tweet.xpath(\".//*[@data-testid='tweetPhoto']/img/@src\").get(),\n            \"video\": tweet.xpath(\".//video/@src\").get(),\n            \"video_thumb\": tweet.xpath(\".//video/@poster\").get(),\n            \"likes\": tweet.xpath(\".//*[@data-testid='like']//text()\").get(),\n            \"retweets\": tweet.xpath(\".//*[@data-testid='retweet']//text()\").get(),\n            \"replies\": tweet.xpath(\".//*[@data-testid='reply']//text()\").get(),\n            \"views\": (tweet.xpath(\".//*[contains(@aria-label,'Views')]\").re(\"(\\d+) Views\") or [None])[0],\n        }\n        # main tweet (not a reply):\n        if i == 0:\n            found[\"views\"] = tweet.xpath('.//span[contains(text(),\"Views\")]/../preceding-sibling::div//text()').get()\n            found[\"retweets\"] = tweet.xpath('.//a[contains(@href,\"retweets\")]//text()').get()\n            found[\"quote_tweets\"] = tweet.xpath('.//a[contains(@href,\"retweets/with_comments\")]//text()').get()\n            found[\"likes\"] = tweet.xpath('.//a[contains(@href,\"likes\")]//text()').get()\n        results.append({k: v for k, v in found.items() if v is not None})\n    return results\n\n\ndef scrape_tweet(url: str, page: Page):\n    \"\"\"\n    Scrape tweet and replies from tweet page like:\n    https://twitter.com/Scrapfly_dev/status/1587431468141318146\n    \"\"\"\n    # go to url\n    page.goto(url)\n    # wait for content to load\n    page.wait_for_selector(\"//article[@data-testid='tweet']\")  \n    # retrieve final page HTML:\n    html = page.content()\n    # parse it for data:\n    selector = Selector(html)\n    tweets = parse_tweets(selector)\n    return tweets\n\n\n# # example run:\n# with sync_playwright() as pw:\n#     # start browser and open a new tab:\n#     browser = pw.chromium.launch(headless=False)\n#     page = browser.new_page(viewport={\"width\": 1920, \"height\": 1080})\n#     # scrape tweet and replies:\n#     tweet_and_replies = scrape_tweet(\"httpTrutwitter.com/Scrapfly_dev/status/1587431468141318146\", page)\n#     print(tweet_and_replies)\n\n\n# example\nfrom playwright.async_api import async_playwright # need to import this first\nfrom gevent import monkey, spawn\nimport asyncio\nimport gevent\n\nmonkey.patch_all()\nloop = asyncio.new_event_loop()\n\n\nasync def f():\n    print(\"start\")\n    playwright = await async_playwright().start()\n    browser = await playwright.chromium.launch(headless=True)\n    context = await browser.new_context()\n    page = await context.new_page()\n    await page.goto(f\"https://www.google.com\")\n    print(\"done\")\n\n\ndef greeny():\n    while True:  # and not other_exit_condition\n        future = asyncio.run_coroutine_threadsafe(f(), loop)\n        while not future.done():\n            gevent.sleep(1)\n\n\ngreenlet1 = spawn(greeny)\ngreenlet2 = spawn(greeny)\n#loop.run_forever()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:56:15.179226Z","iopub.execute_input":"2023-03-13T13:56:15.179644Z","iopub.status.idle":"2023-03-13T13:56:15.903353Z","shell.execute_reply.started":"2023-03-13T13:56:15.179597Z","shell.execute_reply":"2023-03-13T13:56:15.812122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ScrapFly","metadata":{}},{"cell_type":"code","source":"import sys\nprint(sys.getrecursionlimit())","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:56:15.905159Z","iopub.execute_input":"2023-03-13T13:56:15.905831Z","iopub.status.idle":"2023-03-13T13:56:16.097954Z","shell.execute_reply.started":"2023-03-13T13:56:15.905788Z","shell.execute_reply":"2023-03-13T13:56:16.005276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from parsel import Selector\nfrom scrapfly import ScrapflyClient, ScrapeConfig\n\n#scrapfly = ScrapflyClient(key=\"YOUR SCRAPFLY KEY\")\nscrapfly = ScrapflyClient(key=\"scp-live-f35fc29bbb4d476bb1084e6ae155260d\")\n\n\ndef parse_tweets(selector: Selector):\n    \"\"\"\n    parse tweets from pages containing tweets like:\n    - tweet page\n    - search page\n    - reply page\n    - homepage\n    returns list of tweets on the page where 1st tweet is the\n    main tweet and the rest are replies\n    \"\"\"\n    results = []\n    # select all tweets on the page as individual boxes\n    # each tweet is stored under <article data-testid=\"tweet\"> box:\n    tweets = selector.xpath(\"//article[@data-testid='tweet']\")\n    for i, tweet in enumerate(tweets):\n        # using data-testid attribute we can get tweet details:\n        found = {\n            \"text\": \"\".join(tweet.xpath(\".//*[@data-testid='tweetText']//text()\").getall()),\n            \"username\": tweet.xpath(\".//*[@data-testid='User-Names']/div[1]//text()\").get(),\n            \"handle\": tweet.xpath(\".//*[@data-testid='User-Names']/div[2]//text()\").get(),\n            \"datetime\": tweet.xpath(\".//time/@datetime\").get(),\n            \"verified\": bool(tweet.xpath(\".//svg[@data-testid='icon-verified']\")),\n            \"url\": tweet.xpath(\".//time/../@href\").get(),\n            \"image\": tweet.xpath(\".//*[@data-testid='tweetPhoto']/img/@src\").get(),\n            \"video\": tweet.xpath(\".//video/@src\").get(),\n            \"video_thumb\": tweet.xpath(\".//video/@poster\").get(),\n            \"likes\": tweet.xpath(\".//*[@data-testid='like']//text()\").get(),\n            \"retweets\": tweet.xpath(\".//*[@data-testid='retweet']//text()\").get(),\n            \"replies\": tweet.xpath(\".//*[@data-testid='reply']//text()\").get(),\n            \"views\": (tweet.xpath(\".//*[contains(@aria-label,'Views')]\").re(\"(\\d+) Views\") or [None])[0],\n        }\n        # main tweet (not a reply):\n        if i == 0:\n            found[\"views\"] = tweet.xpath('.//span[contains(text(),\"Views\")]/../preceding-sibling::div//text()').get()\n            found[\"retweets\"] = tweet.xpath('.//a[contains(@href,\"retweets\")]//text()').get()\n            found[\"quote_tweets\"] = tweet.xpath('.//a[contains(@href,\"retweets/with_comments\")]//text()').get()\n            found[\"likes\"] = tweet.xpath('.//a[contains(@href,\"likes\")]//text()').get()\n        results.append({k: v for k, v in found.items() if v is not None})\n    return results\n\n\ndef scrape_tweet(url: str):\n    \"\"\"\n    Scrape tweet and replies from tweet page like:\n    https://twitter.com/Scrapfly_dev/status/1587431468141318146\n    \"\"\"\n    result = scrapfly.scrape(ScrapeConfig(\n        url=url,\n        country=\"US\",\n        render_js=True,\n    ))\n    return parse_tweets(result.selector)\n\n\ntweet_and_replies = scrape_tweet(\"https://twitter.com/Google/status/1622686179077357573\")\nprint(tweet_and_replies)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:56:16.100093Z","iopub.execute_input":"2023-03-13T13:56:16.100545Z","iopub.status.idle":"2023-03-13T13:56:16.726173Z","shell.execute_reply.started":"2023-03-13T13:56:16.100503Z","shell.execute_reply":"2023-03-13T13:56:16.631142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scraping Search\n\n- Twitter is known for its powerful search engine and it's a great place to find popular tweets and users.","metadata":{}},{"cell_type":"markdown","source":"- To scrape Twitter search we'll use Playwright as well. Our process looks very similar to our previous scraper:\n\n  1- We'll start a headless Chrome browser\n\n  2- Navigate it to Tweet search page url like https://twitter.com/Scrapfly_dev/status/1587431468141318146\n  \n  3- Wait for the page to load\n\n  4- Retrieve page HTML source\n\n  5- Load HTML to parsel.Selector\n\n  6- Use CSS selectors and XPath to find Tweets or Twitter users\n\n\n  - For this example, we'll cover two search endpoints:\n\n    - People Search - scrape people profiles related to the search query.\n    - Top Results - scrape recommended tweets and profiles related to the search query.","metadata":{}},{"cell_type":"code","source":"from parsel import Selector\nfrom playwright.sync_api import sync_playwright\nfrom playwright.sync_api._generated import Page\n\ndef parse_profiles(sel: Selector):\n    \"\"\"parse profile preview data from Twitter profile search\"\"\"\n    profiles = []\n    for profile in sel.xpath(\"//div[@data-testid='UserCell']\"):\n        profiles.append(\n            {\n                \"name\": profile.xpath(\".//a[not(@tabindex=-1)]//text()\").get().strip(),\n                \"handle\": profile.xpath(\".//a[@tabindex=-1]//text()\").get().strip(),\n                \"bio\": ''.join(profile.xpath(\"(.//div[@dir='auto'])[last()]//text()\").getall()),\n                \"url\": profile.xpath(\".//a/@href\").get(),\n                \"image\": profile.xpath(\".//img/@src\").get(),\n            }\n        )\n    return profiles\n\n\ndef scrape_top_search(query: str, page: Page):\n    \"\"\"scrape top Twitter page for featured tweets\"\"\"\n    page.goto(f\"https://twitter.com/search?q={query}&src=typed_query\")\n    page.wait_for_selector(\"//article[@data-testid='tweet']\")  # wait for content to load\n    tweets = parse_tweets(Selector(page.content()))\n    return tweets\n\n\ndef scrape_people_search(query: str, page: Page):\n    \"\"\"scrape people search Twitter page for related users\"\"\"\n    page.goto(f\"https://twitter.com/search?q={query}&src=typed_query&f=user\")\n    page.wait_for_selector(\"//div[@data-testid='UserCell']\")  # wait for content to load\n    profiles = parse_profiles(Selector(page.content()))\n    return profiles\n\n\n# with sync_playwright() as pw:\n#     browser = pw.chromium.launch(headless=False)\n#     page = browser.new_page(viewport={\"width\": 1920, \"height\": 1080})\n    \n#     top_tweet_search = scrape_top_search(\"google\", page)\n#     people_tweet_search = scrape_people_search(\"google\", page)\n\n\nfrom playwright.async_api import async_playwright # need to import this first\nfrom gevent import monkey, spawn\nimport asyncio\nimport gevent\n\nmonkey.patch_all()\nloop = asyncio.new_event_loop()\n\n\nasync def f():\n    print(\"start\")\n    playwright = await async_playwright().start()\n    browser = await playwright.chromium.launch(headless=True)\n    context = await browser.new_context()\n    page = await context.new_page()\n    await page.goto(f\"https://www.google.com\")\n    print(\"done\")\n\n\ndef greeny():\n    while True:  # and not other_exit_condition\n        future = asyncio.run_coroutine_threadsafe(f(), loop)\n        while not future.done():\n            gevent.sleep(1)\n\n\ngreenlet1 = spawn(greeny)\ngreenlet2 = spawn(greeny)\n#loop.run_forever()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T14:00:06.79702Z","iopub.execute_input":"2023-03-13T14:00:06.798467Z","iopub.status.idle":"2023-03-13T14:00:07.014481Z","shell.execute_reply.started":"2023-03-13T14:00:06.798401Z","shell.execute_reply":"2023-03-13T14:00:06.921251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ScrapFly","metadata":{}},{"cell_type":"code","source":"import json\nfrom parsel import Selector\nfrom playwright.sync_api import sync_playwright\nfrom playwright.sync_api._generated import Page\n#from snippet1 import parse_tweets  # we covered tweet parsing in previous code snippet!\nfrom scrapfly import ScrapflyClient, ScrapeConfig\n\n#scrapfly = ScrapflyClient(key=\"YOUR SCRAPFLY KEY\")\nscrapfly = ScrapflyClient(key=\"scp-live-f35fc29bbb4d476bb1084e6ae155260d\")\n\n\n\ndef parse_profiles(sel: Selector):\n    \"\"\"parse profile preview data from Twitter profile search\"\"\"\n    profiles = []\n    for profile in sel.xpath(\"//div[@data-testid='UserCell']\"):\n        profiles.append(\n            {\n                \"name\": profile.xpath(\".//a[not(@tabindex=-1)]//text()\").get().strip(),\n                \"handle\": profile.xpath(\".//a[@tabindex=-1]//text()\").get().strip(),\n                \"bio\": \"\".join(profile.xpath(\"(.//div[@dir='auto'])[last()]//text()\").getall()),\n                \"url\": profile.xpath(\".//a/@href\").get(),\n                \"image\": profile.xpath(\".//img/@src\").get(),\n            }\n        )\n    return profiles\n\n\ndef scrape_top_search(query: str):\n    \"\"\"scrape top Twitter page for featured tweets\"\"\"\n    result = scrapfly.scrape(\n        ScrapeConfig(\n            url=f\"https://twitter.com/search?q={query}&src=typed_query\",\n            country=\"US\",\n            render_js=True,\n        )\n    )\n    return parse_tweets(result.selector)\n\n\ndef scrape_people_search(query: str):\n    \"\"\"scrape people search Twitter page for related users\"\"\"\n    result = scrapfly.scrape(\n        ScrapeConfig(\n            url=f\"https://twitter.com/search?q={query}&src=typed_query&f=user\",\n            country=\"US\",\n            render_js=True,\n        )\n    )\n    return parse_profiles(result.selector)\n\n\nif __name__ == \"__main__\":\n    top_tweet_search = scrape_top_search(\"google\")\n    print(json.dumps(top_tweet_search, indent=2))\n    people_tweet_search = scrape_people_search(\"google\")\n    print(json.dumps(people_tweet_search, indent=2))","metadata":{"execution":{"iopub.status.busy":"2023-03-13T14:01:32.351435Z","iopub.execute_input":"2023-03-13T14:01:32.352403Z","iopub.status.idle":"2023-03-13T14:01:32.85987Z","shell.execute_reply.started":"2023-03-13T14:01:32.352351Z","shell.execute_reply":"2023-03-13T14:01:32.768441Z"},"trusted":true},"execution_count":null,"outputs":[]}]}