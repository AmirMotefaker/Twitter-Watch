{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/scraping-twitter?scriptVersionId=121986599\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Why Scrape Twitter?\n\n- [Twitter](https://twitter.com/Twitter) is a major announcement hub where people and companies publish their announcements. This is a great opportunity to use Twitter to follow industry trends. For example, stock market or crypto market targets could be scraped to predict the future price of a stock or crypto.\n\n- Twitter is also a great source of data for sentiment analysis. You can use Twitter to find out what people think about a certain topic or brand. This is useful for market research, product development, and brand awareness.\n\n- So, if we can scrape Twitter data with Python we can have access to this valuable public information for free!","metadata":{}},{"cell_type":"markdown","source":"# Setup Wizard\n\n- We'll approach Twitter scraping in three ways:\n\n  - We'll be using [browser automation toolkit Playwright](https://scrapfly.io/blog/web-scraping-with-playwright-and-python/)\n    - This is the easiest way to scrape Twitter as we are using real web browser, so all we have to do is navigate to url, wait for page to load and get the results.\n\n  - We'll also take a look at reverse engineering Twitter's hidden API.\nThis will be a bit harder but these type of scrapers will be much faster than the browser ones. For this we'll be using [httpx](https://pypi.org/project/httpx/).\n\n  - For ScrapFly users we'll also take a look at ScrapFly SDK which makes the above methods even easier.\n\n\n- We'll be working with both JSON and HTML response data. So, we'll be using [parsel](https://pypi.org/project/parsel/) to parse HTML and [jamespath for JSON](https://scrapfly.io/blog/parse-json-jmespath-python/).","metadata":{}},{"cell_type":"markdown","source":"#### All of these libraries are available for free and can be installed via the pip install terminal command:\n\n   $ pip install httpx playwright parsel jmespath scrapfly-sdk","metadata":{}},{"cell_type":"code","source":"!pip install Scrapy\n!pip install httpx playwright parsel jmespath asyncio gevent scrapfly-sdk","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:42:46.603127Z","iopub.execute_input":"2023-03-13T13:42:46.604718Z","iopub.status.idle":"2023-03-13T13:43:25.35144Z","shell.execute_reply.started":"2023-03-13T13:42:46.604648Z","shell.execute_reply":"2023-03-13T13:43:25.349771Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting Scrapy\n  Downloading Scrapy-2.8.0-py2.py3-none-any.whl (272 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.9/272.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting parsel>=1.5.0\n  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\nCollecting queuelib>=1.4.2\n  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\nCollecting zope.interface>=5.1.0\n  Downloading zope.interface-5.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting w3lib>=1.17.0\n  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from Scrapy) (59.8.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from Scrapy) (23.0)\nCollecting PyDispatcher>=2.0.5\n  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\nCollecting itemloaders>=1.0.1\n  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\nCollecting protego>=0.1.15\n  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: lxml>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (4.9.2)\nCollecting itemadapter>=0.1.0\n  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\nCollecting cssselect>=0.9.1\n  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\nCollecting tldextract\n  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting Twisted>=18.9.0\n  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting service-identity>=18.1.0\n  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: pyOpenSSL>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (23.0.0)\nRequirement already satisfied: cryptography>=3.4.6 in /opt/conda/lib/python3.7/site-packages (from Scrapy) (38.0.2)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=3.4.6->Scrapy) (1.15.1)\nRequirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.7/site-packages (from itemloaders>=1.0.1->Scrapy) (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protego>=0.1.15->Scrapy) (1.16.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->Scrapy) (22.2.0)\nRequirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->Scrapy) (0.2.8)\nRequirement already satisfied: pyasn1 in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->Scrapy) (0.4.8)\nCollecting hyperlink>=17.1.1\n  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from Twisted>=18.9.0->Scrapy) (4.4.0)\nCollecting Automat>=0.8.0\n  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\nCollecting constantly>=15.1\n  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\nCollecting incremental>=21.3.0\n  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\nCollecting requests-file>=1.4\n  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\nRequirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tldextract->Scrapy) (2.28.2)\nRequirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from tldextract->Scrapy) (3.4)\nRequirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from tldextract->Scrapy) (3.9.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=3.4.6->Scrapy) (2.21)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->Scrapy) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.1.1)\nInstalling collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, Scrapy\nSuccessfully installed Automat-22.10.0 PyDispatcher-2.0.7 Scrapy-2.8.0 Twisted-22.10.0 constantly-15.1.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 parsel-1.7.0 protego-0.2.1 queuelib-1.6.2 requests-file-1.5.1 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.1.1 zope.interface-5.5.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting httpx\n  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting playwright\n  Downloading playwright-1.31.1-py3-none-manylinux1_x86_64.whl (35.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: parsel in /opt/conda/lib/python3.7/site-packages (1.7.0)\nRequirement already satisfied: jmespath in /opt/conda/lib/python3.7/site-packages (1.0.1)\nCollecting asyncio\n  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gevent\n  Downloading gevent-22.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting scrapfly-sdk\n  Downloading scrapfly_sdk-0.8.5-py3-none-any.whl (28 kB)\nCollecting httpcore<0.17.0,>=0.15.0\n  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.7/site-packages (from httpx) (1.3.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx) (2022.12.7)\nRequirement already satisfied: greenlet==2.0.1 in /opt/conda/lib/python3.7/site-packages (from playwright) (2.0.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from playwright) (4.4.0)\nCollecting pyee==9.0.4\n  Downloading pyee-9.0.4-py2.py3-none-any.whl (14 kB)\nRequirement already satisfied: cssselect>=0.9 in /opt/conda/lib/python3.7/site-packages (from parsel) (1.2.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from parsel) (4.9.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from parsel) (23.0)\nRequirement already satisfied: w3lib>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from parsel) (2.1.1)\nCollecting zope.event\n  Downloading zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\nRequirement already satisfied: zope.interface in /opt/conda/lib/python3.7/site-packages (from gevent) (5.5.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from gevent) (59.8.0)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (1.26.14)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (2.8.2)\nCollecting loguru>=0.5\n  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (1.10.0)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (2.28.2)\nRequirement already satisfied: decorator>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from scrapfly-sdk) (5.1.1)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from httpcore<0.17.0,>=0.15.0->httpx) (3.6.2)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.7/site-packages (from httpcore<0.17.0,>=0.15.0->httpx) (0.14.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->scrapfly-sdk) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.25.0->scrapfly-sdk) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.25.0->scrapfly-sdk) (2.1.1)\nInstalling collected packages: rfc3986, asyncio, zope.event, pyee, loguru, scrapfly-sdk, playwright, httpcore, gevent, httpx\nSuccessfully installed asyncio-3.4.3 gevent-22.10.2 httpcore-0.16.3 httpx-0.23.3 loguru-0.6.0 playwright-1.31.1 pyee-9.0.4 rfc3986-1.5.0 scrapfly-sdk-0.8.5 zope.event-4.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Scraping Tweets\n\n- Twitter is a complicated javascript web application that requires javascript to work. So, for tweet scraping we'll be using [Playwright](https://playwright.dev/) browser automation library.","metadata":{}},{"cell_type":"markdown","source":"#### Playwright-based Twitter scraper in Python should look something like this:\n\n  1- We'll start a headless Chrome browser\n\n  2- Navigate it to Tweet page URL like https://twitter.com/Scrapfly_dev/status/1577664612908077062\n\n  3- Wait for the page to load\n\n  4- Retrieve page HTML source\n\n  5- Load HTML to \"parsel.Selector\"\n\n  6- Use CSS selectors and XPath to extract Tweet details and replies","metadata":{}},{"cell_type":"markdown","source":"# Python","metadata":{}},{"cell_type":"code","source":"from parsel import Selector\nfrom playwright.sync_api import sync_playwright\nfrom playwright.sync_api._generated import Page\n\n\ndef parse_tweets(selector: Selector):\n    \"\"\"\n    parse tweets from pages containing tweets like:\n    - tweet page\n    - search page\n    - reply page\n    - homepage\n    returns list of tweets on the page where 1st tweet is the \n    main tweet and the rest are replies\n    \"\"\"\n    results = []\n    # select all tweets on the page as individual boxes\n    # each tweet is stored under <article data-testid=\"tweet\"> box:\n    tweets = selector.xpath(\"//article[@data-testid='tweet']\")\n    for i, tweet in enumerate(tweets):\n        # using data-testid attribute we can get tweet details:\n        found = {\n            \"text\": \"\".join(tweet.xpath(\".//*[@data-testid='tweetText']//text()\").getall()),\n            \"username\": tweet.xpath(\".//*[@data-testid='User-Names']/div[1]//text()\").get(),\n            \"handle\": tweet.xpath(\".//*[@data-testid='User-Names']/div[2]//text()\").get(),\n            \"datetime\": tweet.xpath(\".//time/@datetime\").get(),\n            \"verified\": bool(tweet.xpath(\".//svg[@data-testid='icon-verified']\")),\n            \"url\": tweet.xpath(\".//time/../@href\").get(),\n            \"image\": tweet.xpath(\".//*[@data-testid='tweetPhoto']/img/@src\").get(),\n            \"video\": tweet.xpath(\".//video/@src\").get(),\n            \"video_thumb\": tweet.xpath(\".//video/@poster\").get(),\n            \"likes\": tweet.xpath(\".//*[@data-testid='like']//text()\").get(),\n            \"retweets\": tweet.xpath(\".//*[@data-testid='retweet']//text()\").get(),\n            \"replies\": tweet.xpath(\".//*[@data-testid='reply']//text()\").get(),\n            \"views\": (tweet.xpath(\".//*[contains(@aria-label,'Views')]\").re(\"(\\d+) Views\") or [None])[0],\n        }\n        # main tweet (not a reply):\n        if i == 0:\n            found[\"views\"] = tweet.xpath('.//span[contains(text(),\"Views\")]/../preceding-sibling::div//text()').get()\n            found[\"retweets\"] = tweet.xpath('.//a[contains(@href,\"retweets\")]//text()').get()\n            found[\"quote_tweets\"] = tweet.xpath('.//a[contains(@href,\"retweets/with_comments\")]//text()').get()\n            found[\"likes\"] = tweet.xpath('.//a[contains(@href,\"likes\")]//text()').get()\n        results.append({k: v for k, v in found.items() if v is not None})\n    return results\n\n\ndef scrape_tweet(url: str, page: Page):\n    \"\"\"\n    Scrape tweet and replies from tweet page like:\n    https://twitter.com/Scrapfly_dev/status/1587431468141318146\n    \"\"\"\n    # go to url\n    page.goto(url)\n    # wait for content to load\n    page.wait_for_selector(\"//article[@data-testid='tweet']\")  \n    # retrieve final page HTML:\n    html = page.content()\n    # parse it for data:\n    selector = Selector(html)\n    tweets = parse_tweets(selector)\n    return tweets\n\n\n# # example run:\n# with sync_playwright() as pw:\n#     # start browser and open a new tab:\n#     browser = pw.chromium.launch(headless=False)\n#     page = browser.new_page(viewport={\"width\": 1920, \"height\": 1080})\n#     # scrape tweet and replies:\n#     tweet_and_replies = scrape_tweet(\"httpTrutwitter.com/Scrapfly_dev/status/1587431468141318146\", page)\n#     print(tweet_and_replies)\n\n\n# example\nfrom playwright.async_api import async_playwright # need to import this first\nfrom gevent import monkey, spawn\nimport asyncio\nimport gevent\n\nmonkey.patch_all()\nloop = asyncio.new_event_loop()\n\n\nasync def f():\n    print(\"start\")\n    playwright = await async_playwright().start()\n    browser = await playwright.chromium.launch(headless=True)\n    context = await browser.new_context()\n    page = await context.new_page()\n    await page.goto(f\"https://www.google.com\")\n    print(\"done\")\n\n\ndef greeny():\n    while True:  # and not other_exit_condition\n        future = asyncio.run_coroutine_threadsafe(f(), loop)\n        while not future.done():\n            gevent.sleep(1)\n\n\ngreenlet1 = spawn(greeny)\ngreenlet2 = spawn(greeny)\n#loop.run_forever()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:43:25.355123Z","iopub.execute_input":"2023-03-13T13:43:25.355687Z","iopub.status.idle":"2023-03-13T13:43:25.962006Z","shell.execute_reply.started":"2023-03-13T13:43:25.35563Z","shell.execute_reply":"2023-03-13T13:43:25.872184Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:80: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/conda/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/conda/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ScrapFly","metadata":{}},{"cell_type":"code","source":"import sys\nprint(sys.getrecursionlimit())","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:43:25.963632Z","iopub.execute_input":"2023-03-13T13:43:25.964276Z","iopub.status.idle":"2023-03-13T13:43:26.146323Z","shell.execute_reply.started":"2023-03-13T13:43:25.964237Z","shell.execute_reply":"2023-03-13T13:43:26.059229Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"3000\n","output_type":"stream"}]},{"cell_type":"code","source":"from parsel import Selector\nfrom scrapfly import ScrapflyClient, ScrapeConfig\n\n#scrapfly = ScrapflyClient(key=\"YOUR SCRAPFLY KEY\")\nscrapfly = ScrapflyClient(key=\"scp-live-f35fc29bbb4d476bb1084e6ae155260d\")\n\n\ndef parse_tweets(selector: Selector):\n    \"\"\"\n    parse tweets from pages containing tweets like:\n    - tweet page\n    - search page\n    - reply page\n    - homepage\n    returns list of tweets on the page where 1st tweet is the\n    main tweet and the rest are replies\n    \"\"\"\n    results = []\n    # select all tweets on the page as individual boxes\n    # each tweet is stored under <article data-testid=\"tweet\"> box:\n    tweets = selector.xpath(\"//article[@data-testid='tweet']\")\n    for i, tweet in enumerate(tweets):\n        # using data-testid attribute we can get tweet details:\n        found = {\n            \"text\": \"\".join(tweet.xpath(\".//*[@data-testid='tweetText']//text()\").getall()),\n            \"username\": tweet.xpath(\".//*[@data-testid='User-Names']/div[1]//text()\").get(),\n            \"handle\": tweet.xpath(\".//*[@data-testid='User-Names']/div[2]//text()\").get(),\n            \"datetime\": tweet.xpath(\".//time/@datetime\").get(),\n            \"verified\": bool(tweet.xpath(\".//svg[@data-testid='icon-verified']\")),\n            \"url\": tweet.xpath(\".//time/../@href\").get(),\n            \"image\": tweet.xpath(\".//*[@data-testid='tweetPhoto']/img/@src\").get(),\n            \"video\": tweet.xpath(\".//video/@src\").get(),\n            \"video_thumb\": tweet.xpath(\".//video/@poster\").get(),\n            \"likes\": tweet.xpath(\".//*[@data-testid='like']//text()\").get(),\n            \"retweets\": tweet.xpath(\".//*[@data-testid='retweet']//text()\").get(),\n            \"replies\": tweet.xpath(\".//*[@data-testid='reply']//text()\").get(),\n            \"views\": (tweet.xpath(\".//*[contains(@aria-label,'Views')]\").re(\"(\\d+) Views\") or [None])[0],\n        }\n        # main tweet (not a reply):\n        if i == 0:\n            found[\"views\"] = tweet.xpath('.//span[contains(text(),\"Views\")]/../preceding-sibling::div//text()').get()\n            found[\"retweets\"] = tweet.xpath('.//a[contains(@href,\"retweets\")]//text()').get()\n            found[\"quote_tweets\"] = tweet.xpath('.//a[contains(@href,\"retweets/with_comments\")]//text()').get()\n            found[\"likes\"] = tweet.xpath('.//a[contains(@href,\"likes\")]//text()').get()\n        results.append({k: v for k, v in found.items() if v is not None})\n    return results\n\n\ndef scrape_tweet(url: str):\n    \"\"\"\n    Scrape tweet and replies from tweet page like:\n    https://twitter.com/Scrapfly_dev/status/1587431468141318146\n    \"\"\"\n    result = scrapfly.scrape(ScrapeConfig(\n        url=url,\n        country=\"US\",\n        render_js=True,\n    ))\n    return parse_tweets(result.selector)\n\n\ntweet_and_replies = scrape_tweet(\"https://twitter.com/Google/status/1622686179077357573\")\nprint(tweet_and_replies)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T13:43:26.149388Z","iopub.execute_input":"2023-03-13T13:43:26.14982Z","iopub.status.idle":"2023-03-13T13:43:26.569991Z","shell.execute_reply.started":"2023-03-13T13:43:26.149783Z","shell.execute_reply":"2023-03-13T13:43:26.568127Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1572734672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtweet_and_replies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://twitter.com/Google/status/1622686179077357573\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_and_replies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/1572734672.py\u001b[0m in \u001b[0;36mscrape_tweet\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"US\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mrender_js\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     ))\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparse_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/backoff/_sync.py\u001b[0m in \u001b[0;36mretry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mmax_tries_exceeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtries\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_tries_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapfly/client.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(self, scrape_config)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mScrapeConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mScrapeApiResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scrapfly/client.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(self, scrape_config)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--> %s Scrapping %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mrequest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scrape_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mscrape_api_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscrape_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscrape_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    585\u001b[0m         }\n\u001b[1;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    497\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m                 )\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m             )\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m             self.ssl_context = create_urllib3_context(\n\u001b[1;32m    396\u001b[0m                 \u001b[0mssl_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolve_ssl_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssl_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 \u001b[0mcert_reqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolve_cert_reqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcert_reqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mcreate_urllib3_context\u001b[0;34m(ssl_version, cert_reqs, options, ciphers)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mOP_NO_TICKET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36moptions\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSSLContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSLContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HOSTFLAG_NEVER_CHECK_SUBJECT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","... last 1 frames repeated, from the frame below ...\n","\u001b[0;32m/opt/conda/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36moptions\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSSLContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSLContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HOSTFLAG_NEVER_CHECK_SUBJECT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"],"ename":"RecursionError","evalue":"maximum recursion depth exceeded","output_type":"error"}]}]}